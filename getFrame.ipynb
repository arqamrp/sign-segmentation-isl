{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddf65f-54a6-420e-b889-c081f9aa71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rgb_video(video_path: Path, fps: int = 25) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Load frames of a video using cv2.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    cap_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    cap_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # cv2 won't be able to change frame rates for all encodings, so we use ffmpeg\n",
    "    if cap_fps != fps:\n",
    "        tmp_video_path = f\"{video_path}.tmp.{video_path.suffix}\"\n",
    "        shutil.move(video_path, tmp_video_path)\n",
    "        cmd = (f\"ffmpeg -i {tmp_video_path} -pix_fmt yuv420p \"\n",
    "               f\"-filter:v fps=fps={fps} {video_path}\")\n",
    "        print(f\"Generating new copy of video with frame rate {fps}\")\n",
    "        os.system(cmd)\n",
    "        Path(tmp_video_path).unlink()\n",
    "        cap = cv2.VideoCapture(str(video_path))\n",
    "        cap_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        cap_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        cap_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        assert cap_fps == fps, f\"ffmpeg failed to produce a video at {fps}\"\n",
    "\n",
    "    f = 0\n",
    "    rgb = []\n",
    "    while True:\n",
    "        # frame: BGR, (h, w, 3), dtype=uint8 0..255\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # BGR (OpenCV) to RGB (Torch)\n",
    "        frame = frame[:, :, [2, 1, 0]]\n",
    "        rgb_t = im_to_torch(frame)\n",
    "        rgb.append(rgb_t)\n",
    "        f += 1\n",
    "    cap.release()\n",
    "    # (nframes, 3, cap_height, cap_width) => (3, nframes, cap_height, cap_width)\n",
    "    rgb = torch.stack(rgb).permute(1, 0, 2, 3)\n",
    "    print(f\"Loaded video {video_path} with {f} frames [{cap_height}hx{cap_width}w] res. \"\n",
    "          f\"at {cap_fps}\")\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b9ae8-380b-4d6b-8d86-ce8122013cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_input(\n",
    "    rgb: torch.Tensor,\n",
    "    resize_res: int = 256,\n",
    "    inp_res: int = 224,\n",
    "    mean: torch.Tensor = 0.5 * torch.ones(3), std=1.0 * torch.ones(3),\n",
    "):\n",
    "    \"\"\"\n",
    "    Process the video:\n",
    "    1) Resize to [resize_res x resize_res]\n",
    "    2) Center crop with [inp_res x inp_res]\n",
    "    3) Color normalize using mean/std\n",
    "    \"\"\"\n",
    "    iC, iF, iH, iW = rgb.shape\n",
    "    rgb_resized = np.zeros((iF, resize_res, resize_res, iC))\n",
    "    for t in range(iF):\n",
    "        tmp = rgb[:, t, :, :]\n",
    "        tmp = resize_generic(\n",
    "            im_to_numpy(tmp), resize_res, resize_res, interp=\"bilinear\", is_flow=False\n",
    "        )\n",
    "        rgb_resized[t] = tmp\n",
    "    rgb = np.transpose(rgb_resized, (3, 0, 1, 2))\n",
    "    # Center crop coords\n",
    "    ulx = int((resize_res - inp_res) / 2)\n",
    "    uly = int((resize_res - inp_res) / 2)\n",
    "    # Crop 256x256\n",
    "    rgb = rgb[:, :, uly : uly + inp_res, ulx : ulx + inp_res]\n",
    "    rgb = to_torch(rgb).float()\n",
    "    assert rgb.max() <= 1\n",
    "    rgb = color_normalize(rgb, mean, std)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774466ef-d00e-4eb8-b4d2-30f8a96bea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sliding_windows(\n",
    "        rgb: torch.Tensor,\n",
    "        num_in_frames: int = 16,\n",
    "        stride: int = 1,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Return sliding windows and corresponding (middle) timestamp\n",
    "    \"\"\"\n",
    "    C, nFrames, H, W = rgb.shape\n",
    "    # If needed, pad to the minimum clip length\n",
    "    if nFrames < num_in_frames:\n",
    "        rgb_ = torch.zeros(C, num_in_frames, H, W)\n",
    "        rgb_[:, :nFrames] = rgb\n",
    "        rgb_[:, nFrames:] = rgb[:, -1].unsqueeze(1)\n",
    "        rgb = rgb_\n",
    "        nFrames = rgb.shape[1]\n",
    "\n",
    "    num_clips = math.ceil((nFrames - num_in_frames) / stride) + 1\n",
    "    plural = \"\"\n",
    "    if num_clips > 1:\n",
    "        plural = \"s\"\n",
    "    print(f\"{num_clips} clip{plural} resulted from sliding window processing.\")\n",
    "\n",
    "    rgb_slided = torch.zeros(num_clips, 3, num_in_frames, H, W)\n",
    "    t_mid = []\n",
    "    # For each clip\n",
    "    for j in range(num_clips):\n",
    "        # Check if num_clips becomes 0\n",
    "        actual_clip_length = min(num_in_frames, nFrames - j * stride)\n",
    "        if actual_clip_length == num_in_frames:\n",
    "            t_beg = j * stride\n",
    "        else:\n",
    "            t_beg = nFrames - num_in_frames\n",
    "        t_mid.append(t_beg + num_in_frames / 2)\n",
    "        rgb_slided[j] = rgb[:, t_beg : t_beg + num_in_frames, :, :]\n",
    "    return rgb_slided, np.array(t_mid), actual_clip_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
